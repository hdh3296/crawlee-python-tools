"""ì›¹ ìŠ¤í¬ë˜í•‘ ëª¨ë“ˆ"""

import asyncio
from typing import Dict, Optional
from datetime import datetime
from crawlee.crawlers import PlaywrightCrawler


class WebScraper:
    """ì›¹í˜ì´ì§€ ìŠ¤í¬ë˜í•‘ í´ë˜ìŠ¤"""
    
    def __init__(self, headless: bool = True, browser_type: str = 'chromium'):
        """ì´ˆê¸°í™”
        
        Args:
            headless: í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ ì—¬ë¶€
            browser_type: ë¸Œë¼ìš°ì € íƒ€ì… ('chromium', 'firefox', 'webkit')
        """
        self.headless = headless
        self.browser_type = browser_type
        self.scraped_data: Optional[Dict] = None
    
    async def scrape(self, url: str) -> Dict:
        """ì›¹í˜ì´ì§€ ìŠ¤í¬ë˜í•‘
        
        Args:
            url: ìŠ¤í¬ë˜í•‘í•  URL
            
        Returns:
            ìŠ¤í¬ë˜í•‘ëœ ë°ì´í„° ë”•ì…”ë„ˆë¦¬
        """
        print(f"ğŸ”„ ìŠ¤í¬ë˜í•‘ ì‹œì‘: {url}")
        
        crawler = PlaywrightCrawler(
            headless=self.headless,
            browser_type=self.browser_type,
            max_requests_per_crawl=1,
        )

        scraped_data = {}

        @crawler.router.default_handler
        async def handler(context):
            nonlocal scraped_data
            
            print("  â³ í˜ì´ì§€ ë¡œë”© ì¤‘...")
            
            # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
            await context.page.wait_for_load_state('load')
            await context.page.wait_for_load_state('domcontentloaded')
            
            # ë©”ì¸ ì½˜í…ì¸  ë¡œë”© ëŒ€ê¸°
            try:
                await context.page.wait_for_selector('main, .content, .document, article, .container', timeout=5000)
                print("  âœ… ë©”ì¸ ì½˜í…ì¸  ë°œê²¬")
            except:
                print("  âš ï¸ ë©”ì¸ ì½˜í…ì¸  ì„ íƒì ëŒ€ê¸° ì‹œê°„ ì´ˆê³¼ (ê³„ì† ì§„í–‰)")
            
            await context.page.wait_for_timeout(3000)
            
            # ì „ì²´ HTML ì¶”ì¶œ
            full_html = await context.page.content()
            page_title = await context.page.title()
            
            scraped_data = {
                'metadata': {
                    'url': str(context.request.url),
                    'page_title': page_title,
                    'timestamp': datetime.now().isoformat(),
                    'content_length': len(full_html)
                },
                'raw_content': {
                    'full_html': full_html,
                }
            }
            
            print(f"  ğŸ“„ HTML í¬ê¸°: {len(full_html):,}ì")
            print(f"  ğŸ“ í˜ì´ì§€ ì œëª©: {page_title}")

        await crawler.run([url])
        
        self.scraped_data = scraped_data
        return scraped_data