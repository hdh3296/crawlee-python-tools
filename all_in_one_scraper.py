#!/usr/bin/env python3
"""
í†µí•© ì›¹í˜ì´ì§€ ìŠ¤í¬ë˜í•‘ & ë§ˆí¬ë‹¤ìš´ ë³€í™˜ ë„êµ¬

ì‚¬ìš©ë²•:
    python3 all_in_one_scraper.py "https://example.com"
    python3 all_in_one_scraper.py  # URL ì…ë ¥ í”„ë¡¬í”„íŠ¸

ê¸°ëŠ¥:
    URL ì…ë ¥ â†’ ìë™ ìŠ¤í¬ë˜í•‘ â†’ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ìƒì„± (ì›ìŠ¤í…)
"""

import asyncio
import sys
import re
import urllib.parse
from datetime import datetime
from pathlib import Path
from crawlee.crawlers import PlaywrightCrawler
from bs4 import BeautifulSoup, NavigableString, Tag


def clean_text(text: str) -> str:
    """í…ìŠ¤íŠ¸ ì •ë¦¬"""
    if not text:
        return ""
    
    # ë³´ì´ì§€ ì•ŠëŠ” íŠ¹ìˆ˜ë¬¸ì ì œê±°
    text = re.sub(r'[\u200B-\u200F\uFEFF\u2060\u00AD]', '', text)  # Zero width characters, BOM, soft hyphen
    text = re.sub(r'[\u0000-\u001F\u007F-\u009F]', '', text)  # Control characters
    text = re.sub(r'[\uE000-\uF8FF]', '', text)  # Private Use Area
    text = re.sub(r'[\uFFF0-\uFFFF]', '', text)  # Specials block
    
    text = text.replace('\t', ' ')
    text = re.sub(r' +', ' ', text)  # ë‹¤ì¤‘ ê³µë°±ì„ ë‹¨ì¼ë¡œ
    return text.strip()


def clean_code_text(text: str) -> str:
    """ì½”ë“œ ë¸”ë¡ í…ìŠ¤íŠ¸ ì •ë¦¬ - ë“¤ì—¬ì“°ê¸° ë³´ì¡´"""
    if not text:
        return ""
    # íƒ­ì„ 4ê°œ ê³µë°±ìœ¼ë¡œ ë³€í™˜í•˜ë˜, ë“¤ì—¬ì“°ê¸°ëŠ” ë³´ì¡´
    text = text.replace('\t', '    ')
    # ì•ë’¤ ê³µë°±ë§Œ ì œê±°í•˜ê³  ë‚´ë¶€ êµ¬ì¡°ëŠ” ìœ ì§€
    return text.strip()


def generate_output_paths(url: str) -> tuple[str, str, str]:
    """URLì„ ê¸°ë°˜ìœ¼ë¡œ ì¶œë ¥ í´ë”ì™€ íŒŒì¼ ê²½ë¡œë“¤ ìƒì„±"""
    parsed = urllib.parse.urlparse(url)
    domain = parsed.netloc.replace('www.', '')
    
    # ë„ë©”ì¸ì—ì„œ íŠ¹ìˆ˜ ë¬¸ì ì œê±°
    clean_domain = re.sub(r'[^\w\-_.]', '_', domain)
    
    # ê²½ë¡œê°€ ìˆìœ¼ë©´ ë§ˆì§€ë§‰ ë¶€ë¶„ ì¶”ê°€
    if parsed.path and parsed.path != '/':
        path_part = parsed.path.strip('/').split('/')[-1]
        path_part = re.sub(r'[^\w\-_.]', '_', path_part)
        if path_part and len(path_part) < 20:
            clean_domain += f"_{path_part}"
    
    # ë‚ ì§œ/ì‹œê°„ ì¶”ê°€
    timestamp = datetime.now().strftime("%Y%m%d_%H%M")
    base_name = f"{clean_domain}_guide_{timestamp}"
    
    # í´ë” êµ¬ì¡° ìƒì„±
    output_dir = Path("outputs")
    output_dir.mkdir(exist_ok=True)
    
    folder_path = output_dir / base_name
    folder_path.mkdir(exist_ok=True)
    
    markdown_path = folder_path / f"{base_name}.md"
    json_path = folder_path / f"{base_name}.json"
    
    return str(folder_path), str(markdown_path), str(json_path)


async def scrape_webpage_direct(url: str) -> dict:
    """ì›¹í˜ì´ì§€ë¥¼ ì§ì ‘ ìŠ¤í¬ë˜í•‘í•˜ì—¬ ë©”ëª¨ë¦¬ì— ë°˜í™˜"""
    
    print(f"ğŸ”„ ìŠ¤í¬ë˜í•‘ ì‹œì‘: {url}")
    
    crawler = PlaywrightCrawler(
        headless=True,
        browser_type='chromium',
        max_requests_per_crawl=1,
    )

    scraped_data = {}

    @crawler.router.default_handler
    async def handler(context):
        nonlocal scraped_data
        
        print("  â³ í˜ì´ì§€ ë¡œë”© ì¤‘...")
        
        # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
        await context.page.wait_for_load_state('load')
        await context.page.wait_for_load_state('domcontentloaded')
        
        # ë©”ì¸ ì½˜í…ì¸  ë¡œë”© ëŒ€ê¸°
        try:
            await context.page.wait_for_selector('main, .content, .document, article, .container', timeout=5000)
            print("  âœ… ë©”ì¸ ì½˜í…ì¸  ë°œê²¬")
        except:
            print("  âš ï¸ ë©”ì¸ ì½˜í…ì¸  ì„ íƒì ëŒ€ê¸° ì‹œê°„ ì´ˆê³¼ (ê³„ì† ì§„í–‰)")
        
        await context.page.wait_for_timeout(3000)
        
        # ì „ì²´ HTML ì¶”ì¶œ
        full_html = await context.page.content()
        page_title = await context.page.title()
        
        scraped_data = {
            'metadata': {
                'url': str(context.request.url),
                'page_title': page_title,
                'timestamp': datetime.now().isoformat(),
                'content_length': len(full_html)
            },
            'raw_content': {
                'full_html': full_html,
            }
        }
        
        print(f"  ğŸ“„ HTML í¬ê¸°: {len(full_html):,}ì")
        print(f"  ğŸ“ í˜ì´ì§€ ì œëª©: {page_title}")

    await crawler.run([url])
    return scraped_data


def process_inline_elements(element):
    """ë¬¸ë‹¨ ë‚´ ì¸ë¼ì¸ ìš”ì†Œë“¤ì„ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ë³€í™˜"""
    result = ""
    
    for child in element.children:
        if isinstance(child, NavigableString):
            result += clean_text(str(child))
        elif isinstance(child, Tag):
            tag_name = child.name.lower()
            text = clean_text(child.get_text())
            
            if tag_name in ['strong', 'b']:
                result += f"**{text}**"
            elif tag_name in ['em', 'i']:
                result += f"*{text}*"
            elif tag_name == 'code':
                result += f"`{text}`"
            else:
                result += text
    
    return clean_text(result)


def extract_all_text_content(element):
    """ìš”ì†Œì—ì„œ ëª¨ë“  í…ìŠ¤íŠ¸ ì½˜í…ì¸ ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì¶”ì¶œ"""
    content_items = []
    
    def process_element(elem, level=0):
        if isinstance(elem, NavigableString):
            text = clean_text(str(elem))
            if text and len(text) > 2:
                content_items.append({
                    'type': 'text',
                    'content': text,
                    'level': level
                })
                return
        
        if isinstance(elem, Tag):
            tag_name = elem.name.lower()
            
            # í—¤ë”© ì²˜ë¦¬
            if tag_name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:
                text = clean_text(elem.get_text())
                if text:
                    content_items.append({
                        'type': 'heading',
                        'level': int(tag_name[1]),
                        'content': text,
                        'tag': tag_name
                    })
                return
            
            # í…Œì´ë¸” ì²˜ë¦¬
            elif tag_name == 'table':
                table_data = extract_table_data(elem)
                if table_data:
                    content_items.append({
                        'type': 'table',
                        'content': table_data
                    })
                return
            
            # ë¦¬ìŠ¤íŠ¸ ì²˜ë¦¬
            elif tag_name in ['ul', 'ol']:
                list_items = extract_list_items(elem)
                if list_items:
                    content_items.append({
                        'type': 'list',
                        'content': list_items,
                        'ordered': tag_name == 'ol'
                    })
                return
            
            # ì½”ë“œ ë¸”ë¡ ì²˜ë¦¬
            elif tag_name in ['pre', 'code']:
                text = clean_code_text(elem.get_text())  # ì½”ë“œ ì „ìš© ì •ë¦¬ í•¨ìˆ˜ ì‚¬ìš©
                if text and len(text) > 5:
                    # pre ì•ˆì˜ codeëŠ” ê±´ë„ˆë›°ê¸°
                    if tag_name == 'code' and elem.parent and elem.parent.name == 'pre':
                        return
                    
                    language = 'javascript' if any(kw in text.lower() for kw in ['var', 'function', 'script', 'eformsign']) else 'text'
                    content_items.append({
                        'type': 'code',
                        'content': text,
                        'language': language
                    })
                return
            
            # Strong/Bold í…ìŠ¤íŠ¸ ì²˜ë¦¬
            elif tag_name in ['strong', 'b']:
                text = clean_text(elem.get_text())
                if text and len(text) > 1:
                    content_items.append({
                        'type': 'bold',
                        'content': text
                    })
                return
            
            # Emphasis/Italic í…ìŠ¤íŠ¸ ì²˜ë¦¬
            elif tag_name in ['em', 'i']:
                text = clean_text(elem.get_text())
                if text and len(text) > 1:
                    content_items.append({
                        'type': 'italic',
                        'content': text
                    })
                return
            
            # ë¬¸ë‹¨ ì²˜ë¦¬ (ì¸ë¼ì¸ ë§ˆí¬ë‹¤ìš´ ì§€ì›)
            elif tag_name == 'p':
                markdown_text = process_inline_elements(elem)
                if markdown_text and len(markdown_text) > 1:
                    content_items.append({
                        'type': 'paragraph',
                        'content': markdown_text
                    })
                return
            
            # ìŠ¤í‚µí•  ìš”ì†Œë“¤
            elif tag_name in ['script', 'style', 'nav', 'header', 'footer']:
                return
            
            # ë‹¤ë¥¸ ëª¨ë“  ìš”ì†Œë“¤ì˜ ìì‹ì„ ì¬ê·€ì ìœ¼ë¡œ ì²˜ë¦¬
            for child in elem.children:
                process_element(child, level + 1)
    
    process_element(element)
    return content_items


def extract_table_data(table_element):
    """í…Œì´ë¸” ë°ì´í„° ì¶”ì¶œ"""
    rows = []
    headers = []
    
    # ëª¨ë“  í–‰ ì°¾ê¸°
    all_rows = table_element.find_all('tr')
    
    for i, row in enumerate(all_rows):
        cells = row.find_all(['th', 'td'])
        if cells:
            cell_texts = [clean_text(cell.get_text()) for cell in cells]
            if any(cell for cell in cell_texts):  # ë¹ˆ í–‰ì´ ì•„ë‹Œ ê²½ìš°
                if i == 0 and row.find('th'):  # ì²« ë²ˆì§¸ í–‰ì´ í—¤ë”ì¸ ê²½ìš°
                    headers = cell_texts
                else:
                    rows.append(cell_texts)
    
    return {'headers': headers, 'rows': rows} if rows else None


def extract_list_items(list_element):
    """ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œ ì¶”ì¶œ"""
    items = []
    list_items = list_element.find_all('li', recursive=False)
    
    for item in list_items:
        text = clean_text(item.get_text())
        if text:
            items.append(text)
    
    return items if items else None


def convert_to_markdown(scraped_data: dict) -> str:
    """ìŠ¤í¬ë˜í•‘ëœ ë°ì´í„°ë¥¼ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ë³€í™˜"""
    
    print("ğŸ”„ ë§ˆí¬ë‹¤ìš´ ë³€í™˜ ì¤‘...")
    
    # HTML íŒŒì‹±
    html_content = scraped_data['raw_content']['full_html']
    soup = BeautifulSoup(html_content, 'html.parser')
    
    # ë©”ì¸ ì½˜í…ì¸  ì˜ì—­ ì°¾ê¸°
    main_area = soup.find('div', class_='document') or soup.find('main') or soup.find('article') or soup.find('body')
    
    if not main_area:
        print("âŒ ë©”ì¸ ì½˜í…ì¸  ì˜ì—­ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return ""
    
    print(f"âœ… ë©”ì¸ ì˜ì—­ ë°œê²¬: {main_area.name}")
    
    # ì½˜í…ì¸  ì¶”ì¶œ
    content_items = extract_all_text_content(main_area)
    print(f"ğŸ“Š {len(content_items)}ê°œ ìš”ì†Œ ì¶”ì¶œ ì™„ë£Œ")
    
    # ë§ˆí¬ë‹¤ìš´ ìƒì„±
    markdown_lines = []
    metadata = scraped_data['metadata']
    
    # ë¬¸ì„œ í—¤ë”
    title = metadata['page_title'] or "ì›¹í˜ì´ì§€ ê°€ì´ë“œ"
    markdown_lines.extend([
        f"# {title}",
        "",
        f"> **ìë™ ìƒì„±ëœ ê°€ì´ë“œ ë¬¸ì„œ**",
        "",
        f"**ì¶œì²˜**: {metadata['url']}",
        f"**ì œëª©**: {metadata['page_title']}",
        f"**ìƒì„±ì¼**: {datetime.now().strftime('%Y-%m-%d %H:%M')}",
        "",
        "---",
        ""
    ])
    
    # ì½˜í…ì¸  ë³€í™˜
    for item in content_items:
        item_type = item['type']
        
        if item_type == 'heading':
            level = item['level']
            content = item['content']
            prefix = '#' * level
            markdown_lines.extend([
                f"{prefix} {content}",
                ""
            ])
        
        elif item_type == 'paragraph' or item_type == 'text':
            markdown_lines.extend([
                item['content'],
                ""
            ])
        
        elif item_type == 'code':
            language = item.get('language', 'text')
            markdown_lines.extend([
                f"```{language}",
                item['content'],
                "```",
                ""
            ])
        
        elif item_type == 'table':
            table_data = item['content']
            
            # í—¤ë”ê°€ ìˆìœ¼ë©´ í—¤ë”ë¶€í„°
            if table_data['headers']:
                header_row = '| ' + ' | '.join(table_data['headers']) + ' |'
                separator = '|' + '---|' * len(table_data['headers'])
                markdown_lines.extend([header_row, separator])
            
            # ë°ì´í„° í–‰ë“¤
            for row in table_data['rows']:
                if row:
                    row_text = '| ' + ' | '.join(str(cell) for cell in row) + ' |'
                    markdown_lines.append(row_text)
            
            markdown_lines.append("")
        
        elif item_type == 'bold':
            markdown_lines.extend([
                f"**{item['content']}**",
                ""
            ])
        
        elif item_type == 'italic':
            markdown_lines.extend([
                f"*{item['content']}*",
                ""
            ])
        
        elif item_type == 'list':
            items = item['content']
            ordered = item.get('ordered', False)
            
            for i, list_item in enumerate(items, 1):
                if ordered:
                    markdown_lines.append(f"{i}. {list_item}")
                else:
                    markdown_lines.append(f"- {list_item}")
            
            markdown_lines.append("")
    
    # í†µê³„ ì •ë³´
    type_counts = {}
    for item in content_items:
        t = item['type']
        type_counts[t] = type_counts.get(t, 0) + 1
    
    markdown_lines.extend([
        "---",
        "",
        "## ğŸ“Š ë¬¸ì„œ ì •ë³´",
        "",
        f"- **ì¶”ì¶œëœ ì´ ìš”ì†Œ**: {len(content_items)}ê°œ",
        f"- **í—¤ë”©**: {type_counts.get('heading', 0)}ê°œ",
        f"- **ë¬¸ë‹¨**: {type_counts.get('paragraph', 0) + type_counts.get('text', 0)}ê°œ",
        f"- **ì½”ë“œ ë¸”ë¡**: {type_counts.get('code', 0)}ê°œ",
        f"- **í…Œì´ë¸”**: {type_counts.get('table', 0)}ê°œ",
        f"- **ë¦¬ìŠ¤íŠ¸**: {type_counts.get('list', 0)}ê°œ",
        "",
        "**âœ… ìë™ ìƒì„± ì™„ë£Œ**: ì›¹í˜ì´ì§€ë¥¼ ì™„ì „íˆ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ë³€í™˜í–ˆìŠµë‹ˆë‹¤.",
        "",
        f"*ìƒì„± ë„êµ¬: all_in_one_scraper.py*",
        f"*ìƒì„± ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*"
    ])
    
    return '\n'.join(markdown_lines)


async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    
    # URL ì…ë ¥ ë°›ê¸°
    if len(sys.argv) > 1:
        url = sys.argv[1]
    else:
        url = input("ğŸ”— ìŠ¤í¬ë˜í•‘í•  URLì„ ì…ë ¥í•˜ì„¸ìš”: ").strip()
    
    if not url:
        print("âŒ URLì´ ì…ë ¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return
    
    # URL ìœ íš¨ì„± ê²€ì‚¬
    if not url.startswith(('http://', 'https://')):
        url = 'https://' + url
    
    try:
        print("=" * 60)
        print("ğŸš€ í†µí•© ì›¹í˜ì´ì§€ ìŠ¤í¬ë˜í•‘ & ë§ˆí¬ë‹¤ìš´ ë³€í™˜ ì‹œì‘")
        print("=" * 60)
        
        # 1ë‹¨ê³„: ìŠ¤í¬ë˜í•‘
        scraped_data = await scrape_webpage_direct(url)
        
        if not scraped_data:
            print("âŒ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨")
            return
        
        print("âœ… ìŠ¤í¬ë˜í•‘ ì™„ë£Œ")
        
        # 2ë‹¨ê³„: ë§ˆí¬ë‹¤ìš´ ë³€í™˜
        markdown_content = convert_to_markdown(scraped_data)
        
        if not markdown_content:
            print("âŒ ë§ˆí¬ë‹¤ìš´ ë³€í™˜ ì‹¤íŒ¨")
            return
        
        print("âœ… ë§ˆí¬ë‹¤ìš´ ë³€í™˜ ì™„ë£Œ")
        
        # 3ë‹¨ê³„: íŒŒì¼ ì €ì¥
        folder_path, markdown_path, json_path = generate_output_paths(url)
        
        # ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ì €ì¥
        with open(markdown_path, 'w', encoding='utf-8') as f:
            f.write(markdown_content)
        
        # JSON íŒŒì¼ ì €ì¥
        import json
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(scraped_data, f, ensure_ascii=False, indent=2)
        
        # íŒŒì¼ ì •ë³´ ìˆ˜ì§‘
        md_size = Path(markdown_path).stat().st_size
        json_size = Path(json_path).stat().st_size
        line_count = len(markdown_content.split('\n'))
        
        print("=" * 60)
        print("ğŸ‰ ë³€í™˜ ì™„ë£Œ!")
        print(f"ğŸ“ ì¶œë ¥ í´ë”: {folder_path}")
        print(f"ğŸ“„ ë§ˆí¬ë‹¤ìš´: {Path(markdown_path).name} ({md_size:,} bytes)")
        print(f"ğŸ“‹ JSON ë°ì´í„°: {Path(json_path).name} ({json_size:,} bytes)")
        print(f"ğŸ“Š ì¤„ ìˆ˜: {line_count:,}")
        print(f"ğŸ”— ì›ë³¸ URL: {url}")
        print("=" * 60)
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())