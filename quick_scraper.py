#!/usr/bin/env python3
"""
ë¹ ë¥¸ ì›¹í˜ì´ì§€ ìŠ¤í¬ë˜í•‘ ë„êµ¬
ì‚¬ìš©ë²•: python3 quick_scraper.py "https://example.com"
"""

import asyncio
import sys
import json
from pathlib import Path
from crawlee.playwright_crawler import PlaywrightCrawler


async def scrape_webpage(url: str, output_name: str = "scraped_result"):
    """ì›¹í˜ì´ì§€ ìŠ¤í¬ë˜í•‘"""
    
    print(f"ğŸ”„ ìŠ¤í¬ë˜í•‘ ì‹œì‘: {url}")
    
    crawler = PlaywrightCrawler(
        headless=True,
        browser_type='chromium',
        max_requests_per_crawl=1,
    )

    @crawler.router.default_handler
    async def handler(context):
        # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
        await context.page.wait_for_load_state('load')
        await context.page.wait_for_load_state('domcontentloaded')
        
        # ë©”ì¸ ì½˜í…ì¸  ë¡œë”© ëŒ€ê¸°
        try:
            await context.page.wait_for_selector('main, .content, .document, article', timeout=5000)
        except:
            pass
        
        await context.page.wait_for_timeout(3000)
        
        # ì „ì²´ HTML ì¶”ì¶œ
        full_html = await context.page.content()
        
        result = {
            'metadata': {
                'url': str(context.request.url),
                'page_title': await context.page.title(),
                'timestamp': context.request.loaded_at.isoformat() if context.request.loaded_at else None,
                'content_length': len(full_html)
            },
            'raw_content': {
                'full_html': full_html,
            }
        }
        
        # JSON íŒŒì¼ë¡œ ì €ì¥
        output_file = f"{output_name}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump([result], f, ensure_ascii=False, indent=2)
        
        print(f"âœ… ìŠ¤í¬ë˜í•‘ ì™„ë£Œ: {output_file}")
        print(f"ğŸ“„ HTML í¬ê¸°: {len(full_html):,}ì")
        print(f"ğŸ“ í˜ì´ì§€ ì œëª©: {await context.page.title()}")

    await crawler.run([url])


def main():
    if len(sys.argv) < 2:
        print("ì‚¬ìš©ë²•: python3 quick_scraper.py 'https://example.com' [ì¶œë ¥íŒŒì¼ëª…]")
        print("ì˜ˆì‹œ: python3 quick_scraper.py 'https://docs.python.org' python_docs")
        return
    
    url = sys.argv[1]
    output_name = sys.argv[2] if len(sys.argv) > 2 else "scraped_result"
    
    asyncio.run(scrape_webpage(url, output_name))
    print(f"\nğŸ¯ ë‹¤ìŒ ë‹¨ê³„:")
    print(f"1. improved_extractor.py íŒŒì¼ì—ì„œ input_fileì„ '{output_name}.json'ìœ¼ë¡œ ìˆ˜ì •")
    print(f"2. python3 improved_extractor.py ì‹¤í–‰")


if __name__ == "__main__":
    main()